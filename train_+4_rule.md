# UNO 训练+4规则说明

注意：在本次迭代中应该尽量减少对原有逻辑的修改，请更多通过新增新函数、新文件，并将接口转向新函数、新文件实现

## 训练内容
1. 在成为+4的受害者时，选择是否挑战+4

## 行为选择
1. 在训练状态下，使用神经网络计算当前挑战成功率，以挑战成功率为概率选择是否挑战
2. 在检验状态下，使用神经网络计算当前挑战成功率，如果挑战成功率大于等于50%，则挑战，否则不挑战

## 训练方法
1. Reward只在返回挑战结果的时候被给予：
如果SimpleAI合法打出+4而RL选择挑战，给予-1.0的reward，并记录为一次错误判断
如果SimpleAI合法打出+4而RL选择不挑战，给予1.0的reward，并记录为一次正确判断
如果SimpleAI非法打出+4而RL选择挑战，给予1.0的reward，并记录为一次正确判断
如果SimpleAI非法打出+4而RL选择不挑战，给予-1.0的reward，并记录为一次错误判断
2. 模型在初始参数下的表现应该为以0.3的概率发起挑战（与SimpleAI相同），这一点请通过运行1000次检验对局，并计算RL发起挑战率检验
3. 为模型配置初始参数的文件应该与训练文件分离，每次运行训练文件时从已有的参数出发，在每进行100局后迭代一次参数

## 训练环境
1. 采用已有的训练环境

## 训练记录
1. 为我创建两个csv文件以记录训练数据
2. 训练中每运行1000个对局后将当前时间、该1000局中RL判断正确次数/判断总次数/判断正确率/胜率打印到控制台，将相同数据记录在第一个csv文件中
3. 训练中每运行50000个对局后将当前时间、该10000局中RL判断正确次数/判断总次数/判断正确率/胜率打印到控制台，将相同数据记录在第二个csv文件中

## 训练成果检验
1. 为我创建一个csv文件以记录检验数据
2. 训练成果通过1个RL，3个SimpleAI的对局检验，运行10000局并计算判断正确率和胜率，将当前时间、判断正确率和胜率记录在csv文件中

## 额外说明
1. 启动虚拟环境请使用source ~/myenv/bin/activate
2. 项目文件夹绝对路径是/home/xiaozy24/python_code/UNO-RL
3. 在找不到某个文件时，请汇报